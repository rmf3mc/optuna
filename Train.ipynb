{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1dd267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Code Starting**\n",
      "**End of Importing**\n"
     ]
    }
   ],
   "source": [
    "print('**Code Starting Optuna Nautilusss**')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from   torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from leafvein2 import Leafvein\n",
    "\n",
    "from backboneModels import *\n",
    "from utils import *\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "print('**End of Importing**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bff512",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reproducility\n",
    "seed=1\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb9c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc=[]\n",
    "train_acc=[]\n",
    "train_total_losses=[]\n",
    "test_total_losses=[]\n",
    "train_iou=[]\n",
    "test_iou=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0327a008",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (3550541012.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [12]\u001b[0;36m\u001b[0m\n\u001b[0;31m    parser.add_argument('--backbone_class', type=str, default='densenet161', choices=['densenet161', 'densenet121',  'resnet50', 'resnet34', 'resnet18', 'mobilenet_v2', 'mobilenet_v3_large' ],'Backbone models')\u001b[0m\n\u001b[0m                                                                                                                                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Create the parser\n",
    "parser = argparse.ArgumentParser(description='Training PyTorch Models For Ultra Fine Grained')\n",
    "\n",
    "# Add arguments\n",
    "parser.add_argument('--lr', default=0.05, type=float, help='learning rate')\n",
    "parser.add_argument('--max_epoch', default=150, type=int)\n",
    "parser.add_argument('--backbone_class', type=str, default='densenet161', choices=['densenet161',  'resnet50', 'resnet34', 'resnet18', 'mobilenet_v2', 'mobilenet_v3_large' ], help='Backbone models')\n",
    "parser.add_argument('--dataset', type=str, default='soybean_2_1', choices=['soybean_2_1', 'btf', 'hainan_leaf'], help='resume from checkpoint')\n",
    "parser.add_argument('--data_dir', type=str, default='./data')\n",
    "parser.add_argument('--seg_size', default=448, type=int, help='Segmentation Dimension')\n",
    "parser.add_argument('--num_classes', default=200, type=int, help='Number of Classes')\n",
    "\n",
    "parser.add_argument('--dataparallel', action='store_true', help='Enable Data Parallel')\n",
    "parser.add_argument('--seg_ild', action='store_true', help='Enable Segmentation Training')\n",
    "parser.add_argument('--cls_ild', action='store_true', help='Enable Classification Training')\n",
    "parser.add_argument('--freeze_all', action='store_true', help='Freeze the Encoder Module')\n",
    "parser.add_argument('--manet', action='store_true', help='Using the MANet')\n",
    "parser.add_argument('--mmanet', action='store_true', help='Using the MMANet')\n",
    "parser.add_argument('--maskguided', action='store_true',help='Guiding the Attention Mask')\n",
    "parser.add_argument('--unet', action='store_true', help='Unet based Segmentation, Unet3+ otherwise')\n",
    "parser.add_argument('--deform_expan', default=3,type=float, help='Applying mean attention to Encoder outputs')\n",
    "\n",
    "parser.add_argument('--model_path', type=str, help='The pretrained model path')\n",
    "parser.add_argument('--fsds', action='store_true', help='Using Full-scale Deep Supervision')\n",
    "\n",
    "parser.add_argument('--local_train', default= 0 , type=int, help='local_training')\n",
    "parser.add_argument('--transfer_to', default= 0 , type=float, help='transfer_to')\n",
    "\n",
    "# Use parse_known_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "print('args.local_train',args.local_train)\n",
    "\n",
    "\n",
    "if args.model_path is not None:\n",
    "    args.batch_size=8\n",
    "    if not args.unet:\n",
    "        args.lr=0.02\n",
    "else:\n",
    "    args.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f56ee7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67bf6db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint/Original_14:19-15-12-2023.pth\n"
     ]
    }
   ],
   "source": [
    "batchsize = args.batch_size\n",
    "MANet=args.manet\n",
    "MMANet=args.mmanet\n",
    "mask_guided=args.maskguided\n",
    "seg_ild=args.seg_ild\n",
    "cls_ild=args.cls_ild\n",
    "freeze_all=args.freeze_all\n",
    "num_classes=args.num_classes\n",
    "model_name=args.backbone_class\n",
    "start_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee2a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/pupil/rmf3mc/Documents/ModelProposing/MGANet/FinalTouches\n",
      "folder_path Original\n",
      "/home/pupil/rmf3mc/Documents/ModelProposing/MGANet/FinalTouches/results/resnet50/Original/14:19-15-12-2023/model_accuracies.txt\n"
     ]
    }
   ],
   "source": [
    "if args.local_train==1:\n",
    "    #current_working_directory = os.getcwd()    \n",
    "    current_working_directory = '/home/pupil/rmf3mc/Documents/ModelProposing/MGANet/FinalTouches_AMP/'\n",
    "else:\n",
    "    current_working_directory = '/mnt/mywork/all_backbones/'\n",
    "print(\"Current Working Directory:\", current_working_directory)\n",
    "\n",
    "name=get_folder_path(args)\n",
    "print('folder_path',name)\n",
    "\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Format the hour:minutes date and time as day-month-year \n",
    "formatted_datetime = current_datetime.strftime(\"%H:%M-%d-%m-%Y\")\n",
    "\n",
    "if cls_ild and not(seg_ild):\n",
    "    train_type=args.dataset+'-results-cls'\n",
    "\n",
    "elif seg_ild and not (cls_ild):\n",
    "    if args.unet:\n",
    "        train_type=os.path.join('optuna',args.dataset+'-results-seg','Unet')\n",
    "    else:\n",
    "        train_type=os.path.join('optuna',args.dataset+'-results-seg','3PlusUnet')\n",
    "\n",
    "else:\n",
    "    if args.unet:\n",
    "        train_type=os.path.join(args.dataset+'-results-seg-cls','Unet')\n",
    "    else:\n",
    "        train_type=os.path.join(args.dataset+'-results-seg-cls','3PlusUnet')\n",
    "\n",
    "folder_path =os.path.join(current_working_directory,train_type,args.backbone_class,name,formatted_datetime)\n",
    "accuracy_file_path =os.path.join(folder_path,'Accuracies_Iou.txt')\n",
    "    \n",
    "    \n",
    "print('File path:',accuracy_file_path)\n",
    "    \n",
    "    \n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    \n",
    "\n",
    "models_folder=current_working_directory+'/checkpoint'\n",
    "if not os.path.exists(models_folder):\n",
    "    os.makedirs(models_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73900e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Leafvein(args,crop=[448,448],hflip=True,vflip=False,erase=False,mode='train')\n",
    "test = Leafvein(args,crop=[448,448],mode='test')\n",
    "model_path= f'{current_working_directory}/optuna/checkpoint/{name}_{formatted_datetime}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "seg_loss_fn   = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iou=0\n",
    "best_acc=0\n",
    "\n",
    "train_best_iou=0\n",
    "best_test_iou=0\n",
    "\n",
    "min_loss=1e10\n",
    "test_acc=0\n",
    "Best_test_acc_BOT=0\n",
    "\n",
    "def objective(trial):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Assuming args is already defined with attributes transfer_to_FSDS, transfer_to, and fsds\n",
    "    group = str(args.transfer_to)+'_FSDS' if args.fsds else str(args.transfer_to)\n",
    "\n",
    "    run=wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"icip24-optuna\",\n",
    "        entity=\"ramytrm\",\n",
    "        group=group,\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"learning_rate\": args.lr,\n",
    "        \"backbone_class\": args.backbone_class,\n",
    "        \"dataset\": args.dataset,\n",
    "        \"Max epochs\": args.max_epoch,\n",
    "        \"Segmentation Training\":args.seg_ild,\n",
    "        \"Classification Training\":args.cls_ild,\n",
    "        \"freeze_all\": args.freeze_all,\n",
    "        \"manet\": args.manet,\n",
    "        \"mmanet\": args.mmanet,\n",
    "        \"maskguided\": args.maskguided,\n",
    "        \"unet\": args.unet,\n",
    "        \"transfer_to\": args.transfer_to,\n",
    "        \"fsds\": args.fsds,\n",
    "        \"local_train\": args.local_train\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    \n",
    "    global model, trainloader, testloader, optimizer\n",
    "    model=MMANET(trial,num_classes=num_classes,MANet=MANet,MMANet=MMANet,mask_guided=mask_guided,seg_included=seg_ild,freeze_all=freeze_all,Unet=args.unet,transform_to=args.transfer_to)\n",
    "    \n",
    "    run.config.deconv_3_no_ch=model.one\n",
    "    run.config.deconv_5_no_ch=model.two\n",
    "    run.config.atrous_2_no_ch=model.three\n",
    "    run.config.atrous_3_no_ch=model.four\n",
    "    run.config.max_mean_no_ch=model.five\n",
    "    \n",
    "    \n",
    "    print('Loading weights')\n",
    "    model_path= args.model_path\n",
    "    model_dict = torch.load(model_path)\n",
    "    state_dict = model_dict['net'] \n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    model=model.to(device)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters in the model: {total_params/1e+6}\")\n",
    "    run.config.model_no_paras=total_params/1e+6\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.max_epoch)\n",
    "    # Get the dataset.\n",
    "    trainloader = DataLoader(train, batch_size=batchsize, shuffle=True)\n",
    "    testloader = DataLoader(test, batch_size=batchsize, shuffle=False)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(args.max_epoch):\n",
    "        start = time.time()        \n",
    "        iou,train_acc,train_ce_loss= train_epoch_Seg(epoch)\n",
    "        curr_test_iou,test_acc,test_ce_loss=test_epoch_Seg(epoch)\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "\n",
    "        print(f'Epoch:{epoch}, Elapsed Time:{end-start}')\n",
    "\n",
    "        trial.report(curr_test_iou, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    run.finish()\n",
    "    return curr_test_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437bf44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = [args.seg_size, args.seg_size]\n",
    "new_size = [size // 2 for size in input_size]\n",
    "\n",
    "def train_epoch_Seg(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    \n",
    "    global new_size, model, trainloader, optimizer\n",
    "   \n",
    "    model.train()\n",
    "    if freeze_all and not(cls_ild):\n",
    "    \n",
    "        for name, module in model.module.features.named_modules():\n",
    "            if isinstance(module, (nn.BatchNorm2d, nn.Dropout)):\n",
    "                module.eval()\n",
    "                \n",
    "                \n",
    "    train_loss = 0\n",
    "    train_ce_loss=0\n",
    "    averageIoU=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    for batch_idx, (inputs, targets, masks) in enumerate(trainloader):\n",
    "        inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "\n",
    "        masks=masks.unsqueeze(1)\n",
    "        masks = F.interpolate(masks, size=new_size, mode='bilinear', align_corners=False)\n",
    "            \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "     \n",
    "        ce_loss_,se_loss_,mse_loss_=0,0,0\n",
    "        \n",
    "        \n",
    "        if mask_guided:\n",
    "            outputs = model(inputs, masks) \n",
    "            msks=outputs['mask']\n",
    "            fg_att=outputs['fg_att']\n",
    "            mse_loss_ = mse(fg_att,msks)\n",
    "\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        if seg_ild:\n",
    "            Final_seg=outputs['Final_seg']\n",
    "            se_loss_ = seg_loss_fn(Final_seg,masks)\n",
    "\n",
    "            preds = torch.sigmoid(Final_seg)\n",
    "            preds = (preds > 0.5).float()\n",
    "\n",
    "            iou = iou_binary(preds, masks)\n",
    "            averageIoU+=iou\n",
    "\n",
    "\n",
    "            out=outputs['out']\n",
    "            ce_loss_ = class_loss_fn(out, targets)\n",
    "\n",
    "            loss =  seg_ild*se_loss_ + cls_ild*ce_loss_+ mask_guided*0.1*mse_loss_\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_ce_loss+=ce_loss_.item()\n",
    "\n",
    "        \n",
    "        _, predicted = out.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        Final_seg,masks,inputs,targets,outputs=None,None,None,None,None\n",
    "    \n",
    "    train_ce_loss/=(batch_idx+1)\n",
    "    train_loss/=(batch_idx+1)\n",
    "        \n",
    "    averageIoU=averageIoU*100/(batch_idx+1)\n",
    "    accuracy=100.*correct/total\n",
    "    \n",
    "    print(f'Acc: {accuracy:.3f}% ({correct}/{total})| CE: {train_ce_loss:.4f}| Total Loss: {train_loss:.4f}| IoU :{averageIoU:.4f}')\n",
    "    \n",
    "    wandb.log({\"Current Training Epoch\": epoch,\n",
    "               \"Training IoU\": averageIoU,\n",
    "               \"Training Accuracy\":accuracy},step=epoch)\n",
    "    train_total_losses.append(train_loss)\n",
    "    train_iou.append(averageIoU)\n",
    "    return averageIoU,accuracy,train_ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch_Seg(epoch):\n",
    "    global best_iou, best_acc, new_size, model, testloader\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_ce_loss = 0\n",
    "    \n",
    "    averageIoU=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets,masks) in enumerate(testloader):\n",
    "            inputs, targets,masks= inputs.to(device), targets.to(device),masks.to(device)\n",
    "            \n",
    "            \n",
    "            masks=masks.unsqueeze(1)\n",
    "            masks = F.interpolate(masks, size=new_size, mode='bilinear', align_corners=False)\n",
    "        \n",
    "\n",
    "            ce_loss_,se_loss_,mse_loss_=0,0,0\n",
    "\n",
    "            if mask_guided:\n",
    "                outputs=model(inputs,masks)\n",
    "                msks=outputs['mask']\n",
    "                fg_att=outputs['fg_att']\n",
    "                mse_loss_ = mse(fg_att,msks)\n",
    "            else:\n",
    "                outputs=model(inputs)\n",
    "                \n",
    "\n",
    "            if seg_ild:\n",
    "                Final_seg=outputs['Final_seg']\n",
    "                se_loss_ = seg_loss_fn(Final_seg,masks)\n",
    "\n",
    "                preds = torch.sigmoid(Final_seg)\n",
    "                preds = (preds > 0.5).float()\n",
    "\n",
    "                iou = iou_binary(preds, masks)\n",
    "                averageIoU+=iou\n",
    "                \n",
    "            out=outputs['out']\n",
    "            ce_loss_ = class_loss_fn(out, targets)\n",
    "\n",
    "            loss =  seg_ild*se_loss_ + cls_ild*ce_loss_+ mask_guided*0.1*mse_loss_\n",
    "\n",
    "\n",
    "\n",
    "            test_ce_loss+=ce_loss_.item()\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            _, predicted = out.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "           \n",
    "            segs,masks,inputs,targets,outputs=None,None,None,None,None\n",
    "\n",
    "    averageIoU=averageIoU*100/(batch_idx+1)\n",
    "    accuracy=100.*correct/total\n",
    "    \n",
    "\n",
    "    if averageIoU>best_iou:\n",
    "        best_iou=averageIoU\n",
    "        \n",
    "    if accuracy>best_acc:\n",
    "        best_acc=accuracy\n",
    "    \n",
    "    test_ce_loss/=(batch_idx+1)\n",
    "    test_loss/=(batch_idx+1)\n",
    "    \n",
    "    test_total_losses.append(test_loss)\n",
    "    test_iou.append(averageIoU)\n",
    "    \n",
    "    print(f'Acc: {accuracy:.3f}% ({correct}/{total})| CE: {test_ce_loss:.4f}|  Total Loss: {test_loss:.4f}| IoU :{averageIoU:.4f}')\n",
    "    print('cur_iou:{0},best_iou:{1}:'.format(averageIoU,best_iou))\n",
    "    print('curr_Acc:{0},best_Acc:{1}:'.format(accuracy,best_acc))\n",
    "    \n",
    "    wandb.log({\"Testing Epoch\": epoch,\n",
    "               \"Current Testing IoU\": averageIoU,\n",
    "               \"Overall Best Testing Iou\":best_iou,\n",
    "               \"Current Testing Accuracy\":accuracy,\n",
    "               \"Best Testing Accuracy\": best_acc},step=epoch)\n",
    "             \n",
    "\n",
    "    return averageIoU,accuracy,test_ce_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0147ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_callback(study, trial):\n",
    "    print(f\"Current value: {trial.value}, Current params: {trial.params}\")\n",
    "    print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd89811",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1319492316.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3835499/1319492316.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    study = optuna.create_study(study_name='test', storage=,direction=\"maximize\", sampler=sampler)\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=1)\n",
    "db_file_path = os.path.join(folder_path, 'study_database.db')\n",
    "study = optuna.create_study(study_name='Study_database', storage=f'sqlite:///{db_file_path}',\n",
    "                            direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=3, timeout=None,  callbacks=[print_callback])\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af87c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "\n",
    "# Your Optuna code here (e.g., defining the study and optimizing it)\n",
    "# ...\n",
    "\n",
    "\n",
    "# Visualization and saving plots as HTML files in the current directory\n",
    "fig_hist = optuna.visualization.plot_optimization_history(study)\n",
    "fig_hist.write_html(os.path.join(folder_path, \"optuna_history.html\"))\n",
    "\n",
    "fig_importance = optuna.visualization.plot_param_importances(study)\n",
    "fig_importance.write_html(os.path.join(folder_path, \"optuna_importance.html\"))\n",
    "\n",
    "fig_edf = optuna.visualization.plot_edf([study])\n",
    "fig_edf.write_html(os.path.join(folder_path, \"optuna_edf.html\"))\n",
    "\n",
    "fig_inter = optuna.visualization.plot_intermediate_values(study)\n",
    "fig_inter.write_html(os.path.join(folder_path, \"optuna_inter.html\"))\n",
    "\n",
    "fig_relation = optuna.visualization.plot_parallel_coordinate(study)\n",
    "fig_relation.write_html(os.path.join(folder_path, \"optuna_relation.html\"))\n",
    "\n",
    "\n",
    "fig_slice = optuna.visualization.plot_slice(study)\n",
    "fig_slice.write_html(os.path.join(folder_path, \"optuna_slice.html\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f1e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_iou=0\n",
    "# best_acc=0\n",
    "\n",
    "# train_best_iou=0\n",
    "# best_test_iou=0\n",
    "\n",
    "# min_loss=1e10\n",
    "# test_acc=0\n",
    "# Best_test_acc_BOT=0\n",
    "\n",
    "# for epoch in range(start_epoch, args.max_epoch):\n",
    "#     start = time.time()\n",
    "#     iou,train_acc,train_ce_loss= train_epoch_Seg(epoch)\n",
    "#     curr_test_iou,test_acc,test_ce_loss=test_epoch_Seg(epoch)\n",
    "#     scheduler.step()\n",
    "#     end = time.time()\n",
    "    \n",
    "#     if seg_ild and not (cls_ild):\n",
    "#         check_seg_performance(iou,curr_test_iou,test_acc,net,epoch,end,start,model_path,accuracy_file_path) \n",
    "#     elif not(seg_ild) and cls_ild:\n",
    "#         check_class_performance(train_ce_loss, test_acc, curr_test_iou,net, epoch, model_path, accuracy_file_path, start, end)\n",
    "#     else:\n",
    "#         check_performance(iou, curr_test_iou, train_ce_loss, test_acc, net, epoch, model_path, accuracy_file_path, start, end)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_seg_performance(iou,curr_test_iou,test_acc,net,epoch,end,start,model_path,accuracy_file_path):\n",
    "#     global train_best_iou,best_test_iou,Best_test_acc_BOT\n",
    "#     if iou>train_best_iou:\n",
    "#         print('Saving..')\n",
    "#         train_best_iou=iou\n",
    "#         best_test_iou=curr_test_iou\n",
    "#         Best_test_acc_BOT=test_acc\n",
    "#         if isinstance(net, torch.nn.DataParallel):\n",
    "#             net_wrap = net.module\n",
    "#         else:\n",
    "#             net_wrap=net\n",
    "#         state = {'net': net_wrap.state_dict(), 'test_iou': best_test_iou, 'Test_acc': test_acc , 'epoch': epoch,}\n",
    "#         torch.save(state, model_path)\n",
    "#     print(f'Best Testing IoU Based On the Training:{best_test_iou}')\n",
    "#     print(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}')\n",
    "#     print(f'Time Elapsed:{end-start}\\n')    \n",
    "#     with open(accuracy_file_path, 'a') as f:\n",
    "#         f.write(f'Best Testing IoU Based On the Training:{best_test_iou}\\n')\n",
    "#         f.write(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}\\n')\n",
    "#         f.write(f'Time Elapsed:{end-start}\\n')\n",
    "        \n",
    "#     wandb.log({\"Best Testing IoU BOT\": best_test_iou,\n",
    "#                \"Best Testing Accuracy BOT\":Best_test_acc_BOT,\n",
    "#                \"Elapsed Time\":end-start},step=epoch)\n",
    "\n",
    "\n",
    "            \n",
    "# def check_class_performance(train_ce_loss, test_acc, curr_test_iou,net, epoch, model_path, accuracy_file_path, start, end):    \n",
    "#     global min_loss, Best_test_acc_BOT, best_test_iou\n",
    "#     if train_ce_loss<min_loss:\n",
    "#         print('Saving..')\n",
    "#         min_loss=train_ce_loss\n",
    "#         Best_test_acc_BOT=test_acc\n",
    "#         best_test_iou=curr_test_iou\n",
    "#         if isinstance(net, torch.nn.DataParallel):\n",
    "#             net_nwrap = net.module\n",
    "#         else:\n",
    "#             net_nwrap=net\n",
    "#         state = {'net': net_nwrap.state_dict(), 'test_iou': curr_test_iou, 'Test_acc': test_acc, 'epoch': epoch,}\n",
    "#         torch.save(state, model_path)\n",
    "    \n",
    "#     print(f'Best Testing IoU Based On the Training:{best_test_iou}')\n",
    "#     print(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}')\n",
    "#     print(f'Time Elapsed:{end-start}\\n')    \n",
    "#     with open(accuracy_file_path, 'a') as f:\n",
    "#         f.write(f'Best Testing IoU Based On the Training:{best_test_iou}\\n')\n",
    "#         f.write(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}\\n')\n",
    "#         f.write(f'Time Elapsed:{end-start}\\n')\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "# def check_performance(iou, curr_test_iou, train_ce_loss, test_acc, net, epoch, model_path, accuracy_file_path, start, end):\n",
    "#     global train_best_iou, best_test_iou, Best_test_acc_BOT, min_loss\n",
    "#     updated = False\n",
    "#     # Check for segment performance improvement\n",
    "#     if iou > train_best_iou:\n",
    "#         print('Saving based on segment performance improvement..')\n",
    "#         train_best_iou = iou\n",
    "#         best_test_iou = curr_test_iou\n",
    "#         Best_test_acc_BOT = test_acc\n",
    "#         updated = True\n",
    "\n",
    "#     # Check for class performance improvement\n",
    "#     if train_ce_loss < min_loss:\n",
    "#         print('Saving based on class performance improvement..')\n",
    "#         min_loss = train_ce_loss\n",
    "#         Best_test_acc_BOT = test_acc\n",
    "#         best_test_iou = curr_test_iou\n",
    "#         updated = True\n",
    "\n",
    "#     # Save the model if there was an update\n",
    "#     if updated:\n",
    "#         if isinstance(net, torch.nn.DataParallel):\n",
    "#             net_nwrap = net.module\n",
    "#         else:\n",
    "#             net_nwrap = net\n",
    "#         state = {'net': net_nwrap.state_dict(),'test_iou': curr_test_iou,'test_acc': test_acc,'epoch': epoch,}\n",
    "#         torch.save(state, model_path)\n",
    "        \n",
    "#     print(f'Best Testing IoU Based On the Training:{best_test_iou}')\n",
    "#     print(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}')\n",
    "#     print(f'Time Elapsed:{end-start}\\n')    \n",
    "#     with open(accuracy_file_path, 'a') as f:\n",
    "#         f.write(f'Best Testing IoU Based On the Training:{best_test_iou}\\n')\n",
    "#         f.write(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}\\n')\n",
    "#         f.write(f'Time Elapsed:{end-start}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.finish()\n",
    "\n",
    "# all_results_file =os.path.join(current_working_directory,train_type,args.backbone_class,'all_results_file.txt')\n",
    "\n",
    "\n",
    "\n",
    "# from filelock import Timeout, FileLock\n",
    "# lock = FileLock(all_results_file[:-4]+'.lock', timeout=120)  # Timeout after two minutes\n",
    "# try:\n",
    "#     with lock:\n",
    "#         with open(all_results_file, 'a') as file:\n",
    "#             file.write(f'\\n*************************************************************\\n')\n",
    "#             file.write(f\"**{name} 's Testing Accuracies**\\n\")\n",
    "#             file.write(f'***Total parameters in the model: {total_params/1e+6}***\\n')            \n",
    "#             file.write(f'*****{formatted_datetime} Time*****\\n')\n",
    "#             file.write('********Training IOU\\n')\n",
    "#             file.write(f'Training IOU:{iou}\\n')\n",
    "#             file.write(f'Best Training IOU:{train_best_iou}\\n')\n",
    "#             file.write('********Testing IOU\\n')\n",
    "#             file.write(f'The overall Very Best Testing IOU :{best_iou}\\n')\n",
    "#             file.write(f'Best Testing IoU Based On the Training:{best_test_iou}\\n')\n",
    "#             file.write('********\\n')\n",
    "#             file.write('Training Acc*****\\n')\n",
    "#             file.write(f'Training Loss:{train_ce_loss}\\n')\n",
    "#             file.write(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}\\n')\n",
    "#             file.write(f'The overall Very Best Testing Accuracy :{best_acc}\\n')\n",
    "#             file.write(f'*************************************************************\\n\\n')\n",
    "# except Timeout:\n",
    "#     print(\"Could not acquire the lock within 120 seconds.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-Unet]",
   "language": "python",
   "name": "conda-env-.conda-Unet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
